# Introduction

Scalable, procedurally generated Markov Decision Processes (MDPs) and Partially Observable MDPs (POMDPs) are created by randomizing the reward matrix, transition matrix, and observation matrix. The library also supports POMDPs with multi-token observation and action spaces.

# Download pre-generated datasets and tasks

- Training DataSet (Large) with 512K sequences and 6B time steps can be downloaded from [here](https://www.kaggle.com/datasets/anonymitynobody/omnirl-training-data-d-large).
- Online evaluating task set and static validation dataset for AnyMDP can be downloaded from [here](https://www.kaggle.com/datasets/anonymitynobody/omnirl-evaluation).

# Install

```bash
pip install xenoverse[anymdp]
```

#### For local installation, execute following commands:

```bash
git clone https://github.com/FutureAGI/xenoverse
cd xenoverse
pip install .[anymdp]
```

# Quick Start

## Import

Import and create the AnyMDP environment with 
```python
import gymnasium as gym
import xenoverse.anymdp

env = gym.make("anymdp-v0", max_steps=5000)
```

## Sampling an AnyMDP task
```python
from xenoverse.anymdp import AnyMDPTaskSampler

task = AnyMDPTaskSampler(
        state_space:int=128,      # size of maximum states
        action_space:int=5,       # number of actions
        min_state_space:int=None, # size of minimum states, default is the same as state_space
        )
env.set_task(task)
env.reset()
```

You might resample a MDP task by keeping the transitions unchanged but sample a new reward matrix by

```python
from xenoverse.anymdp import Resampler
new_task = Resampler(task)
```

## New Features supporting POMDP and multi-token observation and action spaces (2025.12)
```python
    # Test POMDP Task Sampler
    task = AnyPOMDPTaskSampler(state_space=16, 
                            action_space=5,
                            min_state_space=None,
                            observation_space=16,
                            density = 0.1,
                            verbose=True)

    # Test Multi-token POMDP Task Sampler
    # Observation space = MultiDiscrete(observation_tokens)
    # Action space = MultiDiscrete(action_tokens)
    task = MultiTokensAnyPOMDPTaskSampler(state_space=128, 
                            action_space=5,
                            min_state_space=None,
                            observation_space=128,
                            observation_tokens=4,
                            action_tokens=2,
                            density = 0.2,
                            verbose=True)
```
Note that the entry point `env.set_task` is the same for MDP, POMDP, and multi-token POMDP. However, the files `anymdp_solver_mbrl.py` and `anymdp_solver_q.py` are not available for POMDP and multi-token POMDP tasks, while `anymdp_solver_opt.py` is available for all three types of tasks. We provide `test_ppo.py` for demonstration of PPO training on POMDP and multi-token POMDP tasks.

## Running the built-in MDP solver
```python
from xenoverse.anymdp import AnyMDPSolverOpt

solver = AnyMDPSolverOpt(env)  # AnyMDPSolverOpt solves the MDP with ground truth rewards and transition matrix
state, info = env.reset()
terminated, truncated = False, False
while not terminated and not truncated:
    action = solver.policy(state)
    state, reward, terminated, truncated, info = env.step(action)
```

In case you do not want the ground truth rewards and transition to be leaked to the agent, use the AnyMDPSolverOTS instead. This solver inplement a ideal environment modeling and a planning-based policy.

```python
from xenoverse.anymdp import AnyMDPSolverQ, AnyMDPSolverMBRL

 # AnyMDPSolverOTS solves the MDP with dynamic model and dynamic programming
solver = AnyMDPSolverMBRL(env) 

# AnyMDPSolverQ solves the MDP with Tabular Q-learning
#solver = AnyMDPSolverQ(env) 

state, info = env.reset()
terminated, truncated = False, False
while not terminated and not truncated:
    action = solver.policy(state)
    state, reward, terminated, truncated, info = env.step(action)
    solver.learner(state, action, next_state, reward, terminated, truncated) # update the learner
```

## Procedurely generating an MDP task

The transition matrix and reward matrix are procedurally generated by [task_sampler](task_sampler.py). It is generated by randomly sample nodes in a high dimensional space. The transition matrix and the reward matrix are further checked to make sure it is a non-trival problem to be solved.

You can visualize each task by runing ``anymdp_task_visualizer()`` from ``visualizer.py``. It visualizes the markov chain transition kernel and value functions.