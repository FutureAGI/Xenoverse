# Introduction

Scalable procedurally generated Markov Decision Processes (MDPs) by randomizing the reward matrix and transition matrix. 


# Install

```bash
pip install xenoverse[anymdp]
```

#### For local installation, execute following commands:

```bash
git clone https://github.com/FutureAGI/xenoverse
cd xenoverse
pip install .[anymdp]
```

# Quick Start

## Import

Import and create the AnyMDP environment with 
```python
import gymnasium as gym
import xenoverse.anymdp

env = gym.make("anymdp-v0", max_steps=5000)
```

## Sampling an AnyMDP task
```python
from xenoverse.anymdp import AnyMDPTaskSampler

task = AnyMDPTaskSampler(
        state_space:int=128,      # size of maximum states
        action_space:int=5,       # number of actions
        min_state_space:int=None, # size of minimum states, default is the same as state_space
        )
env.set_task(task)
env.reset()
```

You might resample a MDP task by keeping the transitions unchanged but sample a new reward matrix by

```python
from xenoverse.anymdp import Resampler
new_task = Resampler(task)
```

## Running the built-in MDP solver
```python
from xenoverse.anymdp import AnyMDPSolverOpt

solver = AnyMDPSolverOpt(env)  # AnyMDPSolverOpt solves the MDP with ground truth rewards and transition matrix
state, info = env.reset()
terminated, truncated = False, False
while not terminated and not truncated:
    action = solver.policy(state)
    state, reward, terminated, truncated, info = env.step(action)
```

In case you do not want the ground truth rewards and transition to be leaked to the agent, use the AnyMDPSolverOTS instead. This solver inplement a ideal environment modeling and a planning-based policy.

```python
from xenoverse.anymdp import AnyMDPSolverQ, AnyMDPSolverMBRL

 # AnyMDPSolverOTS solves the MDP with dynamic model and dynamic programming
solver = AnyMDPSolverMBRL(env) 

# AnyMDPSolverQ solves the MDP with Tabular Q-learning
#solver = AnyMDPSolverQ(env) 

state, info = env.reset()
terminated, truncated = False, False
while not terminated and not truncated:
    action = solver.policy(state)
    state, reward, terminated, truncated, info = env.step(action)
    solver.learner(state, action, next_state, reward, terminated, truncated) # update the learner
```

## Procedurely generating an MDP task

The transition matrix and reward matrix are procedurally generated by [task_sampler](task_sampler.py). It is generated by randomly sample nodes in a high dimensional space. The transition matrix and the reward matrix are further checked to make sure it is a non-trival problem to be solved.

You can visualize each task by runing ``anymdp_task_visualizer()`` from ``visualizer.py``. It visualizes the markov chain transition kernel and value functions.